How and why you chose the method you did to solve this problem
Over the last few weeks I have done a major amount of research on trying to crack the Substitution Cipher. From my research I figured there was two main ways to attempt this problem using Frequency Analysis. First way would be using letter frequency and the second would be using word frequency. I do believe this a preference in choice, but word frequency stood out more to me and from the beginning I did have many more ideas on how I could solve this using this method. I knew that I was going to need a certain way of iterating over the cipher text multiple times and I felt it was easier to find matches depending on how common English words were rather than letters. Originally, I had a program that used Backtracking, which is â€œalgorithmic-technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point of timeâ€. Being quite honest this program worked pretty well , but there was bugs with it such as : you had to manually change the variable â€œWORD_LENâ€ to a certain integer depending on the length of the cipher file being used e.g. WORD_LEN = 4 , would work on decrypting entire text but only get the decrypted letters from words less or equal to 4 , the issue was if it found a word less or equal to WORD_LEN which wasnâ€™t in â€œDictionary.txtâ€ to be decrypted the program wouldnâ€™t work. Another bug was when I created a daemon thread(runs in the background, it'll automatically end itself when main exits since the thread spawns in the background)I was trying to speed up my code but itâ€™s very hard to implement this with back tracking , so the program might take a second to run depending on how long the sleep function was set for but the output wouldnâ€™t be as efficient (I will insert this backtracking program in to my project , if you wish to analyse it .I needed to figure out a new method that the program will execute its best possible output without changing a variable input nearly every time. This was basically the concept of my actual program: I try a certain amount of words as a possible mapping and compute a score. It carryâ€™s on by building on previous good guesses (high scores) this is done mainly in the â€œget_scoreâ€ function which has my own created formula for conducting a pretty accurate scoring system, I do go into more detail on this scoring system with comments in my program. Something I didnâ€™t implement quite as well in my backtracking program was matching pattern of letters in words e.g. â€œhelloâ€, â€œallâ€, â€œoffâ€ all have reputative letters. This helped me massively in the scoring system to get more accurate decrypting of words and I felt this was the best method to use, as it was reasonably quick and accurate.


How you tested your program to determine which method was faster.
Trying to decide the fastest method was a reoccurring theme for me throughout the entire project. I was determined to implement multiprocessing or threading into my code but concluded that for my program to execute at its fastest, that the implementation of single threaded is a faster method to use. How I tested this was with the cipher files we were already given and I created a program which creates cipher text plus a cipher key, to test many more files (I will insert this cipher creator program in to my project , if you wish to analysis it). Trying to make this faster I learned that multithreading or multiprocessing is complicated and does not necessarily buy you a lot of performance increase. I initially started by checking that I was using the appropriate data structures and the right algorithm. Once I got those important areas working the code was optimized well Youâ€™ll notice in my program I used the built in function â€œpickleâ€, it come in useful as itâ€™s the process of converting an object in memory to a byte stream that can be stored on disk. Also, I created a â€œdatâ€ file from the â€œDictionary.txtâ€ file. All those numbers in the â€œdatâ€ file represent the dictionary of words with the word patterns already computed. This is a way to speed up the script execution, it does not have to load the dictionary or build the dictionary and build the dictionary of word patterns before we can start to decrypt the text. That work has been done and saved to disk. Now we just need to load it and voila. I imported lru_cache, this is handy for freeing up memory space and useful for expensive computed properties of instances that are otherwise effectively immutable. Using simple sets comes in useful too when trying use large chunks of data, saved me a lot of trouble as repeated words can slow the execution time down when checking for thousands of words multiple times and again helped me come up with a method that is faster. After getting my algorithm to work pretty fast, I was then certain that I was going to use single threaded implementation. If you read above the specific code â€œfor idx, match in matches[:20]:â€ I have a comment saying I could of implement multiprocessing here , but didnâ€™t feel the need, like I said in the comment â€œwe have already optimized the code pretty wellâ€

Explain your results. Why do you think you got the answers you did?
In my code under the â€œdecryptâ€ function at the specific line of â€œfor idx, match in matches[:20]:â€. I am slicing the first 20 elements of matches, now this can be changed to a higher or lower integer which more than likely will give different results depending on the integer because you are changing how many possible matches there are in the list. However, I have tested many files with matches set to 20 and overall, I have been happy with these results. Matches will be sorted by how common English words are and this is a key reason why I get certain results. Also, my scoring system obviously plays a huge part on the results when you increase or decrease the weight of the score variable, but when experimenting with different cipher files I think the weight of score and matches slicing first 20 elements is the best for overall results. 

What would you do differently were you to undertake this exercise again?
Overall, I have learned a lot about encryption, impact multiprocessing/multithreading has on code and other built in functions on python. If I were to undertake this exercise again, I would try harder on doing this with a method involving letter frequency and try come up with an idea more involving multiprocessing or multithreading if necessary.
